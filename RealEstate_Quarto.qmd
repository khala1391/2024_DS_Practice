---
title: "RealEstate_Quarto"
format: html
editor: visual
---

# Real Estate - House Price Prediction

### Library

```{r}
# library ----
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

library(tidyverse)
library(TSA)
library(tseries)
library(lubridate)
library(forecast)
library(car)
library(nlme)
library(vars)
library(lmtest)
library(tswge)
library(zoo)
library(combinat)
```

help (explain library)

```{r}
# help for library ----
help(package = "lubridate")
help(package = "TSA")
help(package = "stats")
help(package = "nlme")
help(package = "tseries")
help(package = "vars")
help(package = "lmtest")
help(package = "tswge")
```

help for function

```{r}
# help ----
?base::as.Date
?base::min
?base::max
?base::tapply
?base::expression
?base::diff  # differencing with previous X
?base::cbind

?lubridate::year
?lubridate::month

?stats::ARMAacf  # theoretical data for acf or pacf -? ARMAacf(ar,ma,lag.max, pacf)
?stats::arima.sim  # simulate data for p,q -? arima.sim(list(ar,ma),n)
?stats::acf # plot acf -?  acf(data)
?stats::pacf # plot pacf -? pacf(data)
?stats::ar # fit AR model
?stats::arima  # fit ARIMA model -? arima(data, order=c(p,d,q))

?TSA::eacf # matrix for check p,q -? eacf(data)
?TSA::BoxCox.ar  # transform y -? BoxCox.ar(data)


?stats::ar.yw  # validate ar by yule-walker coefficient

?stats::predict
?stats::frequency  # check no. of observed/unit time # annual: 1, quarterly: 4, monthly: 12
?stats::rstandard  # standardized residual (for diagnosticing model)
?stats::qqnorm
?stats::qqline
?stats::tsdiag  # std. residual, acf residual, p-value Ljung-Box
?stats::AIC
?stats::shapiro.test  # p>0.05 ; normal
?stats::predict.Arima # predict ARIMA model - predict(arima fitted model)
?stats::diffinv # inverse differencing

?tseries::adf.test # check stationary p < 0.05 stationary

?forecast::ets # ets model on y
?forecast::auto.arima # auto fit arima-? auto.arima(data, stepwise, approximation, trace)
?forecast::checkresiduals # check overall p-value from checking white noise
?forecast::tsCV #cross validation fit
?forecast::forecast.ets # -? forecast(obj,h)

?car::durbinWatsonTest # check autocorrelation for residual ONLY lag1 : p>0 => no autocorrelate
?nlme::intervals # compute CI on model 

?lmtest::grangertest # check causality x to y

?vars::VARselect # eda for select lag for VAR fitting
?vars::serial.test  # check Portmanteau test (result still autocorrelate or not) : p >0 no autocorrelate
?vars::causality # check granger test from xi to several xj's in same ts data
?vars::VAR # fit VAR model

?tswge::ljung.wge #Ljung-Box test: residual still autocorrelate : p < 0.05 => autocorrelate

```

# Import Data

```{r}
data <- read.csv('weekly_consolidated_data.csv',header = TRUE)
data$REF_DATE <- as.Date(data$REF_DATE, format = "%Y-%m-%d")
head(data)
tail(data)
```

```{r}
# Recheck data
H <- 52 # frequency as weekly
par(mfrow=c(1,1))

# EDA
min(data$REF_DATE)
max(data$REF_DATE)
```

```{r}
# Convert each column to zoo time series
data_ts_list <- list(
  Wage       = zoo(data$Wage, order.by = data$REF_DATE),
  Cement     = zoo(data$Cement, order.by = data$REF_DATE),
  Energy     = zoo(data$Energy, order.by = data$REF_DATE),
  Lumber     = zoo(data$Lumber, order.by = data$REF_DATE),
  Metal      = zoo(data$Metal, order.by = data$REF_DATE),
  HousePrice = zoo(data$HOUSE_PRICE_IDX, order.by = data$REF_DATE)
)
```

## Data Variables

```{r, fig.height=6, fig.width=12}
# Set plotting layout: 2 rows x 3 columns
par(mfrow = c(2, 3), mar = c(4, 4, 3, 1))  # adjust margins

# Loop to plot each variable
for(name in names(data_ts_list)) {
  plot(
    data_ts_list[[name]],
    ylab = name,
    xlab = "Year",
    main = paste(name, "time series"),
    type = "l",
    col = "blue"
  )
}
```

# Univariate

## Data Manipulate

```{r}
### Handle with Lubridate 
### Convert to ts objects (weekly) ##

data_ts <- zoo(data$HOUSE_PRICE_IDX, order.by = data$REF_DATE)
start(data_ts)
end(data_ts)
```

## Split Train and Test

```{r}
nTotal <- length(data_ts)

nTest <- 104        # e.g., last 2 year for test
nTrain <- nTotal - nTest

time_points <- time(data_ts)
end_train  <- time_points[nTrain]           # last train point
start_test <- time_points[nTrain + 1]       # first test point

data_ts_train <- window(data_ts, end = end_train)
data_ts_test  <- window(data_ts, start = start_test)
```

```{r}
# Using zoo for weekly data spliting
# convert zoo to ts
data_ts_train <- ts(coredata(data_ts_train),
                    start = c(year(index(data_ts_train)[1]), 
                              week(index(data_ts_train)[1])),
                    frequency = H)

data_ts_test <- ts(coredata(data_ts_test),
                   start = c(year(index(data_ts_test)[1]), 
                             week(index(data_ts_test)[1])),
                   frequency = H)


# Check
start(data_ts_train); end(data_ts_train)
start(data_ts_test); end(data_ts_test)
```

## EDA: Train

```{r}
# Time Series plot
plot(data_ts_train, ylab="House Price Index", xlab="year", main="house price history")
```

#### BoxCox: Transformation checking

```{r}
# See BoxCox
BoxCox.ar(data_ts)
 # lambda = 1 , No transformation needed
```

```{r}
# Find proper lambda
x_train <- as.numeric(data_ts_train)
lambda <- BoxCox.lambda(x_train)
lambda
```

-   Lambda around 1, no need to transform

#### ADF: Stationary Checking

```{r}
# Stationary test
adf.test(data_ts_train) # p < 0.05 ==> stationary
```

-   **H₀: series is non-stationary**

-   **H₁: series is stationary**

    The ADF test strongly rejects the null hypothesis of a unit root (p \< 0.01). This means data is stationary, so it is suitable for ARIMA modeling.

## 1. Modeling: ETS -\> ETS(A,A,N) (Test: MAPE 2.628%, RMSE

## %, RMSE 3.695)

```{r, fig.height=12}
plot(decompose(data_ts_train))
```

```{r}
ets_model <- ets(data_ts_train)# lm object  , let R select model type
summary(ets_model) # A,Ad,N
ets_model
plot(ets(data_ts_train))  # ignore seasonal
lines(data_ts_test, col = "red")
```

The ETS(A,A,N) model (Holt’s linear trend) fits the house price index well. There is a smooth upward trend, updated slowly over time (small α and β), capturing the long-term growth of the housing market. Residuals show almost no autocorrelation, and forecast errors are small (MAPE \~3.6%), indicating a well-behaved model. Because weekly seasonality cannot be modeled by ETS, the model fitting ignored seasonality — which is appropriate since the data does not exhibit clear weekly seasonal patterns.

#### Cross Validation

```{r}
# cross-validation # : overlay on data_ts_test ----
ets_forecast_cross <- forecast(ets_model,h = nTest)
accuracy(ets_forecast_cross$mean,data_ts_test)
plot(ets_forecast_cross)
lines(data_ts_test, col = "orange", lwd = 1, lty = 2)
```

## 2. Modeling: ARIMA(0,1,2) (Test: MAPE 2.131%, RMSE 2.986)

```{r fig.height= 12}
# Plot ACF, PACF from train

par(mfrow = c(2,1))

# --- ACF ---
acf_out <- acf(data_ts_train, plot = FALSE, lag.max = 260)
lags_acf <- acf_out$lag[,1,1] * frequency(data_ts_train)

plot(
  lags_acf,acf_out$acf,type = "h",
  xlab = "Lag",ylab = "ACF",main = "ACF for house price index")

abline(h = c(-1,1) * qnorm(0.975) / sqrt(length(data_ts_train)), 
       col = "blue", lty = 2)

# --- PACF ---
pacf_out <- pacf(data_ts_train, plot = FALSE, lag.max = 260)
lags_pacf <- pacf_out$lag * frequency(data_ts_train)

plot(
  lags_pacf,pacf_out$acf,type = "h",
  xlab = "Lag",ylab = "Partial ACF",
  main = "PACF for house price index")

abline(h = c(-1,1) * qnorm(0.975) / sqrt(length(data_ts_train)), 
       col = "blue", lty = 2)

```

-   Non stationary: The original house price index is highly persistent and clearly non-stationary, with ACF near 1 across many lags and a strong PACF spike at lag 1; therefore the series must be differenced before fitting an ARIMA model.

```{r fig.height= 12}
# Plot ACF, PACF from train

par(mfrow = c(2,1))
dif <- diff(data_ts_train, 1)

# --- ACF ---
acf_out <- acf(dif, plot = FALSE, lag.max = 260)
lags_acf <- acf_out$lag[,1,1] * frequency(data_ts_train)

plot(
  lags_acf,acf_out$acf,type = "h",
  xlab = "Lag",ylab = "ACF",main = "ACF for house price index at diff=1")

abline(h = c(-1,1) * qnorm(0.975) / sqrt(length(data_ts_train)), 
       col = "blue", lty = 2)

# --- PACF ---
pacf_out <- pacf(dif, plot = FALSE, lag.max = 260)
lags_pacf <- pacf_out$lag * frequency(data_ts_train)

plot(
  lags_pacf,pacf_out$acf,type = "h",
  xlab = "Lag",ylab = "PACF",
  main = "PACF for house price index at diff=1")

abline(h = c(-1,1) * qnorm(0.975) / sqrt(length(data_ts_train)), 
       col = "blue", lty = 2)
```

-   Driff = 1: After first differencing, the series becomes stationary with a strong negative MA(1) signature—ACF cuts off after lag 1 while PACF decays—indicating that ARIMA(0,1,1) is the appropriate model structure.

```{r}
# Loop through each combination of exogenous vars

# ===== Parameter grid =====
p_values <- 0:3
q_values <- 0:3
d_value  <- 1              # <-- change if needed (0,1,2...)

# Create combinations
arima_grid <- expand.grid(p = p_values,
                          d = d_value,
                          q = q_values)

# Empty results table
df_results <- data.frame(
  p = integer(),
  d = integer(),
  q = integer(),
  RMSE = numeric(),
  MAE = numeric(),
  MAPE = numeric(),
  MASE = numeric(),
  AIC = numeric(),
  stringsAsFactors = FALSE
)

# ===== Loop through all combinations =====
for(i in 1:nrow(arima_grid)) {
  
  p <- arima_grid$p[i]
  d <- arima_grid$d[i]
  q <- arima_grid$q[i]
  
  cat("Fitting ARIMA(", p, ",", d, ",", q, ")\n")
  
  # Fit ARIMA
  fit <- tryCatch(
    Arima(data_ts_train, order = c(p, d, q)),
    error = function(e) NULL
  )
  
  if(is.null(fit)) {
    cat(" --> Model failed. Skipping.\n")
    next
  }
  
  # Forecast
  fc <- forecast(fit, h = nTest)
  
  # Compute accuracy
  acc <- accuracy(fc$mean, data_ts_test)
  row_index <- if(nrow(acc) >= 2) 2 else 1
  
  RMSE <- acc[row_index, "RMSE"]
  MAE  <- acc[row_index, "MAE"]
  MAPE <- acc[row_index, "MAPE"]
  AIC  <- fit$aic
  

  # Append to results
  df_results <- rbind(df_results, data.frame(
    p = p,
    d = d,
    q = q,
    RMSE = RMSE,
    MAE  = MAE,
    MAPE = MAPE,
    AIC  = AIC,
    stringsAsFactors = FALSE
  ))
}

```

```{r}
# ===== Sort results by AIC =====
df_results_sorted_AIC <- df_results[order(df_results$AIC), ]
print(df_results_sorted_AIC)

# Create empty AIC matrix
aic_matrix <- matrix(NA, 
                      nrow = length(p_values), 
                      ncol = length(q_values),
                      dimnames = list(
                        paste0("p=", p_values),
                        paste0("q=", q_values)
                      ))

# Fill matrix with AIC
for (i in 1:nrow(df_results)) {
  p <- df_results$p[i]
  q <- df_results$q[i]
  AIC <- df_results$AIC[i]
  
  aic_matrix[p + 1, q + 1] <- AIC   # +1 because index starts at 1
}

# Print AIC matrix
aic_matrix
```

```{r}
# ===== Sort results by RMSE =====
df_results_sorted_RMSE <- df_results[order(df_results$RMSE), ]
print(df_results_sorted_RMSE)

# Create empty RMSE matrix
rmse_matrix <- matrix(NA, 
                      nrow = length(p_values), 
                      ncol = length(q_values),
                      dimnames = list(
                        paste0("p=", p_values),
                        paste0("q=", q_values)
                      ))

# Fill matrix with RMSE
for (i in 1:nrow(df_results)) {
  p <- df_results$p[i]
  q <- df_results$q[i]
  RMSE <- df_results$RMSE[i]
  
  rmse_matrix[p + 1, q + 1] <- RMSE   # +1 because index starts at 1
}

# Print RMSE matrix
rmse_matrix
```

### 2.1 Auto ARIMA -\> ARIMA(2,1,2) (Test: MAPE 2.154%, RMSE 2.986)

```{r}
# ARIMA model ----
auto_arima <- auto.arima(data_ts_train,
                         stepwise = T,
                         seasonal = F,
                         approximation = F,
                         trace = T)
```

```{r}
# Summary Best Auto ARIMA
summary(auto_arima)
```

#### Cross Validation

```{r}
## cross -validation # : overlay on data_ts_test ----
auto_arima_forecast_cross <- forecast(auto_arima,h = nTest)
accuracy(auto_arima_forecast_cross$mean,data_ts_test)
plot(auto_arima_forecast_cross)
lines(data_ts_test, col = "orange", lwd = 1, lty = 2)
```

#### Normality of Standard Residuals

```{r}
## normality of (standardized) residuals ####

par(mfrow=c(1,2))
hist(rstandard(auto_arima),xlab="Standardised residuals")
qqnorm(rstandard(auto_arima))
qqline(rstandard(auto_arima))
```

```{r}
shapiro.test(rstandard(auto_arima))
```

```{r}
## Alternatively you can checkresiduals()
# It will give you Ljung-Box test for overall pattern (Portmanteau tests )
checkresiduals(auto_arima)
```

```{r fig.height=12}
## ACF of residuals ####
tsdiag(auto_arima,gof.lag=20)
```

-   Normal \>\> explain more about this test

### 2.2 Manually Choose Parameter

-   Try ARIMA(2,1,2), ARIMA(0,1,1) and ARIMA(0,1,2) -\> less parameter with less AIC

### 2.2.1 Try ARIMA(0,1,1) -\> ARIMA(0,1,1) (Test: MAPE 2.132%, RMSE 2.989)

from ACF, PACF suggestion

```{r}
arima_011 <- Arima(data_ts_train, order = c(0,1,1))
summary(arima_011)
```

#### Cross Validation

```{r}
## cross -validation # : overlay on data_ts_test ----
arima_011_forecast_cross <- forecast(arima_011,h = nTest)
accuracy(arima_011_forecast_cross$mean,data_ts_test)
plot(arima_011_forecast_cross)
lines(data_ts_test, col = "orange", lwd = 1, lty = 2)
```

#### Normality of Standard Residuals

```{r}
par(mfrow=c(1,2))
hist(rstandard(arima_011),xlab="Standardised residuals")
qqnorm(rstandard(arima_011))
qqline(rstandard(arima_011))
```

```{r}
shapiro.test(rstandard(arima_011))
```

```{r}
## Alternatively you can checkresiduals()
# It will give you Ljung-Box test for overall pattern (Portmanteau tests )
checkresiduals(arima_011)
```

```{r fig.height=12}
## ACF of residuals ####
tsdiag(arima_011,gof.lag=20)
```

-   Normal \>\> explain more about this test too

### 2.2.2 Try ARIMA(0,1,2) -\> ARIMA(0,1,2) (Test: MAPE 2.131%, RMSE 2.986)

less parameter with quite less AIC

```{r}
arima_012 <- Arima(data_ts_train, order = c(0,1,2))
summary(arima_012)
```

#### Cross Validation

```{r}
## cross -validation # : overlay on data_ts_test ----
arima_012_forecast_cross <- forecast(arima_012,h = nTest)
accuracy(arima_012_forecast_cross$mean,data_ts_test)
plot(arima_012_forecast_cross)
lines(data_ts_test, col = "orange", lwd = 1, lty = 2)
```

#### Normality of Standard Residuals

```{r}
par(mfrow=c(1,2))
hist(rstandard(arima_012),xlab="Standardised residuals")
qqnorm(rstandard(arima_012))
qqline(rstandard(arima_012))
```

```{r}
shapiro.test(rstandard(arima_012))
```

```{r}
## Alternatively you can checkresiduals()
# It will give you Ljung-Box test for overall pattern (Portmanteau tests )
checkresiduals(arima_012)
```

```{r fig.height=12}
tsdiag(arima_012,gof.lag=20)
```

-   Normal \>\> explain more about this test

## 3. Modeling: SARIMA(0,1,1)(0,0,1)\[52\] (Test: MAPE 2.801%, RMSE 3.937)

### 3.1 Auto SARIMA -\> SARIMA(0,1,1)(0,0,1)\[52\] (Test: MAPE 2.801%, RMSE 3.937)

```{r}
auto_sarima <- auto.arima(data_ts_train,
                         stepwise = T,
                         seasonal = T,
                         approximation = F,
                         trace = T)
```

```{r}
summary(auto_sarima)
```

#### Cross Validation

```{r}
# cross-validation # : overlay on data_ts_test
auto_sarima_forecast_cross <- forecast(auto_sarima,h = nTest)
accuracy(auto_sarima_forecast_cross$mean,data_ts_test)
plot(auto_sarima_forecast_cross)
lines(data_ts_test, col = "orange", lwd = 1, lty = 2)
```

#### Normality of Standard Residual

```{r}
# cross-validation # : overlay on data_ts_test
auto_sarima_forecast_cross <- forecast(auto_sarima,h = nTest)
accuracy(auto_sarima_forecast_cross$mean,data_ts_test)
plot(auto_sarima_forecast_cross)
lines(data_ts_test, col = "red")
```

```{r}
## normality of (standardized) residuals ####

par(mfrow=c(1,2))
hist(rstandard(auto_sarima),xlab="Standardised residuals")
qqnorm(rstandard(auto_sarima))
qqline(rstandard(auto_sarima))
```

```{r}
shapiro.test(rstandard(auto_sarima))
```

```{r}
## Alternatively you can checkresiduals()
# It will give you Ljung-Box test for overall pattern (Portmanteau tests )
checkresiduals(auto_sarima)
```

```{r fig.height=12}
## ACF of residuals ####
tsdiag(auto_sarima,gof.lag=20)
```

-   Normal \>\> explain more about this test

# Multivariate

## Data Manipulate

```{r}
# Check
str(data)
head(data)
tail(data)
names(data)


y_zoo  <- zoo(data$HOUSE_PRICE_IDX, order.by = data$REF_DATE)

X_zoo <- zoo(data[, c("Wage","Cement","Energy","Lumber","Metal")],
             order.by = data$REF_DATE)
```

```{r}

```

## Granger Causality Test

```{r}
# 1. Extract y and X as numeric vectors

y  <- as.numeric(y_zoo)

X1 <- as.numeric(X_zoo[, "Wage"])
X2 <- as.numeric(X_zoo[, "Cement"])
X3 <- as.numeric(X_zoo[, "Energy"])
X4 <- as.numeric(X_zoo[, "Lumber"])
X5 <- as.numeric(X_zoo[, "Metal"])
```

```{r}
# 2. Put into a simple data frame for grangertest

df_g <- data.frame(
  y     = y,
  Wage  = X1,
  Cement = X2,
  Energy = X3,
  Lumber = X4,
  Metal  = X5
)
```

```{r}
# Function to run bidirectional Granger test

run_granger <- function(df, y_col, x_col, maxlag = 4) {
  
  cat("\n----------------------------------------------\n")
  cat("Testing:", x_col, "→", y_col, "(Does X cause Y?)\n")
  print(grangertest(as.formula(paste(y_col, "~", x_col)),
                    order = maxlag, data = df))
  
  cat("\nTesting:", y_col, "→", x_col, "(Does Y cause X?)\n")
  print(grangertest(as.formula(paste(x_col, "~", y_col)),
                    order = maxlag, data = df))
}


# 3. Run Granger test for all exogenous variables

vars <- c("Wage","Cement","Energy","Lumber","Metal")

for (v in vars) {
  run_granger(df_g, "y", v, maxlag = 4)
}
```

-   **Wage and y:** Bidirectional Granger causality (Wage ↔ y).

-   **Cement and y:** No Granger causality in either direction.

-   **Energy and y:** Unidirectional causality (Energy → y).

-   **Lumber and y:** No Granger causality in either direction.

-   **Metal and y:** Bidirectional Granger causality (Metal ↔ y).

```{r}
# Candidate exogenous variables
exog_vars <- c("Wage", "Energy", "Metal")
```

## Split Train and Test

```{r}
## Train / Test Split
nTotal     <- length(y_zoo)
# nTest must already be defined above
nTrain     <- nTotal - nTest

time_points <- time(y_zoo)

end_train  <- time_points[nTrain]
start_test <- time_points[nTrain + 1]


## Y split

y_train <- window(y_zoo, end = end_train)
y_test  <- window(y_zoo, start = start_test)


## X split (same index)

X_train <- window(X_zoo, end = end_train)
X_test  <- window(X_zoo, start = start_test)
```

```{r}
## Convert zoo → ts

to_ts <- function(z) {
  ts(coredata(z),
     start = c(year(index(z)[1]), week(index(z)[1])),
     frequency = H)
}

y_train_ts    <- to_ts(y_train)
y_test_ts     <- to_ts(y_test)

X_train_sel    <- X_train[, selected_vars]
X_test_sel     <- X_test[, selected_vars]

X_train_ts <- ts(coredata(X_train_sel),
                 start = c(year(index(X_train_sel)[1]),
                           week(index(X_train_sel)[1])),
                 frequency = H)

X_test_ts <- ts(coredata(X_test_sel),
                start = c(year(index(X_test_sel)[1]),
                          week(index(X_test_sel)[1])),
                frequency = H)
```

```{r}
## Build List for use

data_tsx_house <- list(
  y_train = y_train_ts,
  y_test = y_test_ts,
  
  X_train = X_train_ts,
  X_test = X_test_ts
)
```

```{r}
## Check

head(data_tsx_house$y_train)
tail(data_tsx_house$y_train)
head(data_tsx_house$y_test)
tail(data_tsx_house$y_test)

head(data_tsx_house$X_train)
tail(data_tsx_house$X_train)
head(data_tsx_house$X_test)
tail(data_tsx_house$X_test)
```

## 4. Modeling: ARIMAX(2,1,2)+Energy (Test MAPE 2.142%, RMSE 2.971)

```{r}
# Generate all non-empty combinations ----
all_combos <- unlist(
  lapply(1:length(exog_vars), function(k) combn(exog_vars, k, simplify = FALSE)),
  recursive = FALSE
)
```

```{r}
# Prepare results storage with AIC
df_results <- data.frame(
  exog = character(),
  RMSE = numeric(),
  MAE  = numeric(),
  MAPE = numeric(),
  MASE = numeric(),
  AIC  = numeric(),
  stringsAsFactors = FALSE
)
```

```{r}
# Loop through each combination
for(vars in all_combos) {
  
  cat("Fitting ARIMAX with:", paste(vars, collapse = "+"), "\n")
  
  # Prepare xreg for train and test 
  X_train_combo <- data_tsx_house$X_train[, vars, drop = FALSE]
  X_test_combo  <- data_tsx_house$X_test[, vars, drop = FALSE]
  
  # Fit ARIMAX
  fit <- auto.arima(
    data_tsx_house$y_train,
    xreg = X_train_combo,
    stepwise = TRUE,
    seasonal = FALSE,
    approximation = FALSE,
    trace = FALSE
  )
  
  # Forecast on test set 
  n_test <- length(data_tsx_house$y_test)
  fc <- forecast(fit, h = n_test, xreg = X_test_combo)
  
  # Compute accuracy safely
  acc <- accuracy(fc, data_tsx_house$y_test)
  row_index <- if(nrow(acc) >= 2) 2 else 1
  
  RMSE <- if("RMSE" %in% colnames(acc)) acc[row_index, "RMSE"] else NA
  MAE  <- if("MAE"  %in% colnames(acc)) acc[row_index, "MAE"] else NA
  MAPE <- if("MAPE" %in% colnames(acc)) acc[row_index, "MAPE"] else NA
  MASE <- if("MASE" %in% colnames(acc)) acc[row_index, "MASE"] else NA
  AIC  <- fit$aic  # extract AIC from fitted model
  
  # Append results
  df_results <- rbind(df_results, data.frame(
    exog = paste(vars, collapse = "+"),
    RMSE = RMSE,
    MAE  = MAE,
    MAPE = MAPE,
    MASE = MASE,
    AIC  = AIC,
    stringsAsFactors = FALSE
  ))
}
```

```{r}
# Sort results by RMSE or AIC as needed
df_results_sorted <- df_results[order(df_results$RMSE), ]
print(df_results_sorted)
```

### 4.1 Try Min RMSE: ARIMAX(2,1,2) + Energy (Test MAPE 2.142%, RMSE 2.971)

```{r}
# Select best combination
best_row <- df_results_sorted[1, ]
cat("Best exogenous variables (by RMSE):", best_row$exog, "\n")
best_vars <- unlist(strsplit(as.character(best_row$exog), "\\+"))

X_train_best <- data_tsx_house$X_train[, best_vars, drop = FALSE]
X_test_best  <- data_tsx_house$X_test[, best_vars, drop = FALSE]

```

```{r}
# Fit ARIMAX with best exogenous variables
auto_arimax_best <- auto.arima(
  data_tsx_house$y_train,
  xreg = X_train_best,
  stepwise = TRUE,
  seasonal = FALSE,
  approximation = FALSE
)
```

```{r}
summary(auto_arimax_best)
```

#### Cross Validation

```{r}
# Forecast on test
forecast_test  <- forecast(
  auto_arimax_best,
  h = length(data_tsx_house$y_test),
  xreg = X_test_best)

accuracy(forecast_test$mean, data_tsx_house$y_test)
plot(forecast_test)
```

```{r}
# Standardized residuals histogram and QQ plot
par(mfrow=c(1,2))
hist(rstandard(auto_arimax_best), xlab="Standardized residuals", main="Residuals Histogram")
qqnorm(rstandard(auto_arimax_best))
qqline(rstandard(auto_arimax_best))

```

```{r}
# Shapiro-Wilk test for normality
shapiro.test(rstandard(auto_arimax_best))
```

```{r}
# Alternatively, use checkresiduals() for full diagnostic
checkresiduals(auto_arimax_best)
```

```{r fig.height=12}
## ACF of residuals ####
tsdiag(auto_arimax_best,gof.lag=20)
```

-   Normal \>\> explain more about this test

### 4.2 Try 2nd Min RMSE: ARIMAX(2,1,2) + Energy +Metal (Test: MAPE 2.170%, RMSE 3.015)

```{r}
# Select best combination
best_row_mape <- df_results_sorted[2, ]
cat("Best exogenous variables (by MAPE):", best_row_mape$exog, "\n")
best_vars_mape <- unlist(strsplit(as.character(best_row_mape$exog), "\\+"))

X_train_best_mape <- data_tsx_house$X_train[, best_vars_mape, drop = FALSE]
X_test_best_mape  <- data_tsx_house$X_test[, best_vars_mape, drop = FALSE]
```

```{r}
# Fit ARIMAX with best exogenous variables
auto_arimax_best_mape <- auto.arima(
  data_tsx_house$y_train,
  xreg = X_train_best_mape,
  stepwise = TRUE,
  seasonal = FALSE,
  approximation = FALSE
)
```

```{r}
summary(auto_arimax_best_mape)
```

#### Cross Validation

```{r}
# Forecast on test
forecast_test  <- forecast(
  auto_arimax_best_mape,
  h = length(data_tsx_house$y_test),
  xreg = X_test_best_mape)

accuracy(forecast_test$mean, data_tsx_house$y_test)
plot(forecast_test)
```

#### Normality of Standard Residual

```{r}
# Standardized residuals histogram and QQ plot
par(mfrow=c(1,2))
hist(rstandard(auto_arimax_best_mape), xlab="Standardized residuals", main="Residuals Histogram")
qqnorm(rstandard(auto_arimax_best_mape))
qqline(rstandard(auto_arimax_best_mape))
```

```{r}
# Shapiro-Wilk test for normality
shapiro.test(rstandard(auto_arimax_best_mape))
```

```{r}
# Alternatively, use checkresiduals() for full diagnostic
checkresiduals(auto_arimax_best_mape)
```

```{r fig.height=12}
tsdiag(auto_arimax_best,gof.lag=20)
```

-   Normal \>\> explain more about this test

## (No need to Present) 5. Model: SARIMAX

```{r}
# Loop through each combination
for(vars in all_combos) {
  
  cat("Fitting SARIMAX with:", paste(vars, collapse = "+"), "\n")
  
  # Prepare xreg for train and test 
  X_train_combo <- data_tsx_house$X_train[, vars, drop = FALSE]
  X_test_combo  <- data_tsx_house$X_test[, vars, drop = FALSE]
  
  # Fit SARIMAX
  fit <- auto.arima(
    data_tsx_house$y_train,
    xreg = X_train_combo,
    stepwise = TRUE,
    seasonal = TRUE,
    approximation = FALSE,
    trace = FALSE
  )
  
  # Forecast on test set 
  n_test <- length(data_tsx_house$y_test)
  fc <- forecast(fit, h = n_test, xreg = X_test_combo)
  
  # Compute accuracy safely
  acc <- accuracy(fc, data_tsx_house$y_test)
  row_index <- if(nrow(acc) >= 2) 2 else 1
  
  RMSE <- if("RMSE" %in% colnames(acc)) acc[row_index, "RMSE"] else NA
  MAE  <- if("MAE"  %in% colnames(acc)) acc[row_index, "MAE"] else NA
  MAPE <- if("MAPE" %in% colnames(acc)) acc[row_index, "MAPE"] else NA
  MASE <- if("MASE" %in% colnames(acc)) acc[row_index, "MASE"] else NA
  AIC  <- fit$aic  # extract AIC from fitted model
  
  # Append results
  df_results <- rbind(df_results, data.frame(
    exog = paste(vars, collapse = "+"),
    RMSE = RMSE,
    MAE  = MAE,
    MAPE = MAPE,
    MASE = MASE,
    AIC  = AIC,
    stringsAsFactors = FALSE
  ))
}
```

```{r}
# Sort results by RMSE or AIC as needed
df_results_sorted <- df_results[order(df_results$RMSE), ]
print(df_results_sorted)
```

### 5.1 Try Min RMSE: SARIMAX(0,1,1)(0,01)\[52\] (Test MAPE 2.455%, RMSE 3.464)

```{r}
# Select best combination
best_row <- df_results_sorted[1, ]
cat("Best exogenous variables (by RMSE):", best_row$exog, "\n")
best_vars <- unlist(strsplit(as.character(best_row$exog), "\\+"))
```

```{r}
X_train_best <- data_tsx_house$X_train[, best_vars, drop = FALSE]
X_test_best  <- data_tsx_house$X_test[, best_vars, drop = FALSE]

```

```{r}
# Fit SARIMAX with best exogenous variables
auto_sarimax_best <- auto.arima(
  data_tsx_house$y_train,
  xreg = X_train_best,
  stepwise = TRUE,
  seasonal = TRUE,
  approximation = FALSE
)
summary(auto_sarimax_best)
```

#### Cross Validation

```{r}
# Forecast on test
forecast_test  <- forecast(
  auto_sarimax_best,
  h = length(data_tsx_house$y_test),
  xreg = X_test_best)

accuracy(forecast_test$mean, data_tsx_house$y_test)
plot(forecast_test)
```

#### Normality of Standard Residual

```{r}
# Standardized residuals histogram and QQ plot
par(mfrow=c(1,2))
hist(rstandard(auto_sarimax_best), xlab="Standardized residuals", main="Residuals Histogram")
qqnorm(rstandard(auto_sarimax_best))
qqline(rstandard(auto_sarimax_best))
```

```{r}
# Shapiro-Wilk test for normality
shapiro.test(rstandard(auto_sarimax_best))
```

```{r}
# Alternatively, use checkresiduals() for full diagnostic
checkresiduals(auto_sarimax_best)
```

```{r fig.height=12}
tsdiag(auto_sarimax_best,gof.lag=20)
```

-   Normal \>\> explain more about this test

# Select Best Model (Min MAPE): ARIMA(0,1,2) (Test: MAPE 2.131%, RMSE 2.986)

```{r}
# Best model on Train
summary(arima_012)
```

```{r}
# forecast through test (nTest) + forecast 2025 (52 weeks)
h <- nTest + 52

arima_012_forecast_full <- forecast(arima_012, h = h)
ts_time <- time(arima_012_forecast_full$mean)

# find which indices are still in 2025
idx_2025 <- which(ts_time < 2026 & ts_time >= 2025)

# final h for 2025
h_2025 <- max(idx_2025)

# final forecast that ends exactly in 2025
fc_2025 <- forecast(arima_012, h = h_2025)
```

```{r}
start(data_ts_train)
end(data_ts_train)
start(data_ts_test)
end(data_ts_test)
```

```{r}
# Check
tail(fc_2025$mean)
```

```{r fig.width=8}
# ---- 1. Plot baseline forecast ----
plot(fc_2025,
     main = "Forecast for 2025",
     ylab = "House Price Index",
     xlab = "Year")

# add test data
lines(data_ts_test, col = "orange", lty = 2)

# add legend
legend(
  "topleft",
  legend = c("Train Data", "Test Data"),
  col    = c("black", "orange"),
  lty    = c(1, 2),
  lwd    = c(2, 2),
  bty    = "n"
)

# ---- 2. Helper positions ----
usr <- par("usr")     # plot boundaries: (x1, x2, y1, y2)

# positions for annotation (5% below top)
y_top <- usr[4] - 0.05 * (usr[4] - usr[3])

# ---- VERTICAL LINES ----

# (1) end of training set
vline_train <- time(data_ts_train)[length(data_ts_train)]
abline(v = vline_train, col = "orange", lwd = 2, lty = 2)

# (2) end of actual test data
vline_test <- time(data_ts_test)[length(data_ts_test)]
abline(v = vline_test, col = "darkgreen", lwd = 2, lty = 2)

# ---- X positions for zone labels ----
x_center_train   <- mean(time(data_ts_train))
x_center_test    <- mean(time(data_ts_test))
x_center_forecast <- mean(c(vline_test, max(time(fc_2025$mean))))

# ---- 3. Add annotation labels ----

text(x = x_center_train, y = y_top,
     labels = "Train",
     col = "black", cex = 0.8, font = 2)

text(x = x_center_test, y = y_top,
     labels = "Test",
     col = "orange", cex = 0.8, font = 2)

# allow drawing outside plot boundary
par(xpd = NA)


# small shift to the right of vline_test (2% of x-range)
x_shift <- 0.02 * (usr[2] - usr[1])
x_forecast_label <- vline_test + x_shift
# 
# # y position OUTSIDE the plot (slightly above top border)
# y_outside <- usr[4] + 0.05 * (usr[4] - usr[3])
# 
# annotation outside plot area, left-justified
text(x = x_forecast_label,
     y = y_top,
     labels = "Forecast",
     col = "darkgreen",
     cex = 0.8,
     font = 2,
     adj = 0)

```

```{r}
# Check
tail(data_ts_test, 1)
```

```{r}
# House Price Prediction
tail(fc_2025$mean, 1)
```

-   GPT: Since the model is **ARIMA(0,1,2)**, long-range forecasts tend to be stable and flat, meaning the predicted value stays close to the most recent 2024 levels. This forecast reflects the baseline expected movement without assuming any sudden shocks or unusual market changes. The number 115.14 should be interpreted relative to your index base (e.g., if the index is normalized to 100 in a base year, then 115.14 means **\~15% higher than the base level**).

```{r}
# Compare to 2024
last_actual_2024  <- as.numeric(tail(data_ts_test, 1))
last_forecast_2025 <- as.numeric(tail(fc_2025$mean, 1))

# --- Calculate differences ---
abs_change <- last_forecast_2025 - last_actual_2024
pct_change <- (abs_change / last_actual_2024) * 100

# --- Print summary ---
cat("Latest Actual (2024):   ", round(last_actual_2024, 4), "\n")
cat("Latest Forecast (2025): ", round(last_forecast_2025, 4), "\n\n")

cat("Absolute Change:        ", round(abs_change, 4), "\n")
cat("Percentage Change:      ", round(pct_change, 4), "%\n")
```

-   GPT: House Price Index is expected to increase slightly from 114.34 at the end of 2024 to 115.14 at the end of 2025. This represents a small increase of about 0.8 index points, or roughly +0.7% growth over the year.
